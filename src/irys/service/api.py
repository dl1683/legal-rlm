"""FastAPI REST API for Irys RLM service."""

import asyncio
import logging
import os
import time
import uuid
from contextlib import asynccontextmanager
from dataclasses import asdict
from datetime import datetime
from pathlib import Path
from typing import Optional

import aiofiles
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
import httpx

from .config import ServiceConfig, get_config
from .models import (
    InvestigateRequest,
    InvestigateResponse,
    JobResult,
    JobStatus,
    SearchRequest,
    SearchResponse,
    SearchResult,
    HealthResponse,
    ErrorResponse,
    UploadInvestigateResponse,
    UploadSearchResponse,
    SyncInvestigateResponse,
    S3UrlsInvestigateRequest,
    S3UrlsSearchRequest,
)
from .s3_repository import S3Repository

logger = logging.getLogger(__name__)

# In-memory job storage (use Redis in production for multi-worker)
_jobs: dict[str, JobResult] = {}
_start_time: float = time.time()

# Version
VERSION = "1.0.0"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    config = get_config()

    # Validate config on startup
    errors = config.validate()
    if errors and not config.debug:
        for error in errors:
            logger.error(f"Config error: {error}")

    # Create temp directory
    Path(config.temp_dir).mkdir(parents=True, exist_ok=True)
    logger.info(f"Irys RLM Service v{VERSION} starting...")
    logger.info(f"S3 Bucket: {config.s3_bucket}")
    logger.info(f"Temp Dir: {config.temp_dir}")

    # Start cleanup task
    cleanup_task = asyncio.create_task(_cleanup_loop(config))

    yield

    # Cleanup on shutdown
    cleanup_task.cancel()
    logger.info("Shutting down...")


async def _cleanup_loop(config: ServiceConfig):
    """Background task to clean up expired temp files."""
    while True:
        await asyncio.sleep(60)  # Check every minute
        try:
            # Clean old jobs from memory
            now = datetime.now()
            expired = [
                job_id for job_id, job in _jobs.items()
                if job.completed_at and
                (now - job.completed_at).seconds > config.cleanup_after_seconds
            ]
            for job_id in expired:
                del _jobs[job_id]
                logger.debug(f"Cleaned up job {job_id}")
        except Exception as e:
            logger.error(f"Cleanup error: {e}")


def create_app(config: Optional[ServiceConfig] = None) -> FastAPI:
    """Create FastAPI application."""
    config = config or get_config()

    app = FastAPI(
        title="Irys RLM API",
        description="Legal document investigation service with recursive language model",
        version=VERSION,
        lifespan=lifespan,
        docs_url="/docs",
        redoc_url="/redoc",
    )

    # CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # Configure for production
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Store config in app state
    app.state.config = config

    return app


# Create default app instance
app = create_app()


def _serialize_result(result) -> tuple[list, dict]:
    """Convert InvestigationResult citations and entities to serializable dicts."""
    # Convert citations (list of Citation dataclasses)
    citations = []
    for c in result.citations:
        try:
            d = asdict(c)
            # Convert datetime to ISO string
            if 'timestamp' in d and d['timestamp']:
                d['timestamp'] = d['timestamp'].isoformat()
            citations.append(d)
        except Exception:
            citations.append({"error": "Failed to serialize citation"})

    # Convert entities (dict of str -> Entity dataclasses)
    entities = {}
    for name, entity in result.entities.items():
        try:
            entities[name] = asdict(entity)
        except Exception:
            entities[name] = {"error": "Failed to serialize entity"}

    return citations, entities


# === ENDPOINTS ===


@app.get("/health", response_model=HealthResponse, tags=["System"])
async def health_check():
    """Health check endpoint."""
    config = get_config()

    # Check S3 connection
    s3_connected = False
    if config.s3_bucket:
        try:
            import boto3
            s3 = boto3.client("s3", region_name=config.s3_region)
            s3.head_bucket(Bucket=config.s3_bucket)
            s3_connected = True
        except Exception:
            pass

    # Check Gemini connection
    gemini_connected = bool(config.gemini_api_key)

    # Calculate temp storage usage
    temp_dir = Path(config.temp_dir)
    temp_size_mb = 0.0
    if temp_dir.exists():
        temp_size_mb = sum(
            f.stat().st_size for f in temp_dir.rglob("*") if f.is_file()
        ) / (1024 * 1024)

    # Count active jobs
    active_jobs = sum(
        1 for job in _jobs.values()
        if job.status in (JobStatus.PENDING, JobStatus.PROCESSING)
    )

    return HealthResponse(
        status="healthy",
        version=VERSION,
        gemini_connected=gemini_connected,
        s3_connected=s3_connected,
        active_jobs=active_jobs,
        temp_storage_mb=round(temp_size_mb, 2),
        uptime_seconds=round(time.time() - _start_time, 2),
    )


@app.post(
    "/investigate",
    response_model=InvestigateResponse,
    tags=["Investigation"],
    responses={400: {"model": ErrorResponse}},
)
async def start_investigation(
    request: InvestigateRequest,
    background_tasks: BackgroundTasks,
):
    """Start a new document investigation.

    Downloads documents from S3, runs investigation using Gemini,
    and returns results asynchronously.
    """
    config = get_config()

    # Check concurrent job limit
    active_count = sum(
        1 for job in _jobs.values()
        if job.status in (JobStatus.PENDING, JobStatus.PROCESSING)
    )
    if active_count >= config.max_concurrent_jobs:
        raise HTTPException(
            status_code=429,
            detail=f"Too many concurrent jobs. Max: {config.max_concurrent_jobs}",
        )

    # Generate job ID
    job_id = f"inv_{uuid.uuid4().hex[:12]}"

    # Create job record
    job = JobResult(
        job_id=job_id,
        status=JobStatus.PENDING,
        query=request.query,
        s3_prefix=request.s3_prefix,
        created_at=datetime.now(),
    )
    _jobs[job_id] = job

    # Start background investigation
    background_tasks.add_task(
        _run_investigation,
        job_id,
        request,
        config,
    )

    return InvestigateResponse(
        job_id=job_id,
        status=JobStatus.PENDING,
        message="Investigation started",
        estimated_seconds=60,
    )


async def _run_investigation(
    job_id: str,
    request: InvestigateRequest,
    config: ServiceConfig,
):
    """Background task to run investigation."""
    job = _jobs[job_id]
    job.status = JobStatus.PROCESSING
    s3_repo = None
    temp_dir = None

    try:
        # Download documents from S3
        s3_repo = S3Repository(
            bucket=config.s3_bucket,
            prefix=request.s3_prefix,
            config=config,
        )
        temp_dir = await s3_repo.download_to_temp(job_id)

        # Run investigation
        from irys import Irys
        irys = Irys(api_key=config.gemini_api_key)

        result = await irys.investigate(
            query=request.query,
            repository=str(temp_dir),
        )

        # Extract results
        job.analysis = result.output
        job.citations, job.entities = _serialize_result(result)
        job.documents_processed = result.state.documents_read
        job.status = JobStatus.COMPLETED
        job.completed_at = datetime.now()
        job.duration_seconds = (
            job.completed_at - job.created_at).total_seconds()

        logger.info(f"Job {job_id} completed in {job.duration_seconds:.1f}s")

        # Call webhook if provided
        if request.callback_url:
            await _send_callback(request.callback_url, job)

    except Exception as e:
        logger.error(f"Job {job_id} failed: {e}")
        job.status = JobStatus.FAILED
        job.error = str(e)
        job.completed_at = datetime.now()

    finally:
        # Cleanup temp files
        if s3_repo and temp_dir:
            await s3_repo.cleanup(job_id)


async def _send_callback(url: str, job: JobResult):
    """Send results to callback URL."""
    try:
        async with httpx.AsyncClient() as client:
            await client.post(
                url,
                json=job.model_dump(mode="json"),
                timeout=30,
            )
            logger.info(f"Sent callback for job {job.job_id}")
    except Exception as e:
        logger.error(f"Callback failed for job {job.job_id}: {e}")


@app.get(
    "/investigate/{job_id}",
    response_model=JobResult,
    tags=["Investigation"],
    responses={404: {"model": ErrorResponse}},
)
async def get_investigation(job_id: str):
    """Get investigation status and results."""
    if job_id not in _jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    return _jobs[job_id]


@app.get("/jobs", response_model=list[JobResult], tags=["Investigation"])
async def list_jobs(
    status: Optional[JobStatus] = None,
    limit: int = 20,
):
    """List recent investigation jobs."""
    jobs = list(_jobs.values())

    if status:
        jobs = [j for j in jobs if j.status == status]

    # Sort by created_at descending
    jobs.sort(key=lambda j: j.created_at, reverse=True)
    return jobs[:limit]


@app.post(
    "/search",
    response_model=SearchResponse,
    tags=["Search"],
)
async def quick_search(request: SearchRequest):
    """Quick keyword search across documents."""
    config = get_config()
    job_id = f"search_{uuid.uuid4().hex[:8]}"

    try:
        # Download documents
        s3_repo = S3Repository(
            bucket=config.s3_bucket,
            prefix=request.s3_prefix,
            config=config,
        )
        temp_dir = await s3_repo.download_to_temp(job_id)

        # Run search
        from irys import quick_search as irys_search
        results = await irys_search(
            query=request.query,
            repository=str(temp_dir),
            api_key=config.gemini_api_key,
        )

        # Cleanup
        await s3_repo.cleanup(job_id)

        return SearchResponse(
            results=[SearchResult(**r) for r in results[:request.max_results]],
            total_matches=len(results),
            documents_searched=len(list(temp_dir.rglob("*"))),
        )

    except Exception as e:
        logger.error(f"Search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# === FILE UPLOAD ENDPOINTS ===


async def _save_uploaded_files(
    files: list[UploadFile],
    temp_dir: Path,
) -> int:
    """Save uploaded files to temp directory. Returns count saved."""
    saved = 0
    for file in files:
        if not file.filename:
            continue
        # Sanitize filename
        filename = Path(file.filename).name
        dest_path = temp_dir / filename
        async with aiofiles.open(dest_path, "wb") as f:
            content = await file.read()
            await f.write(content)
        saved += 1
    return saved


@app.post(
    "/upload/investigate",
    response_model=UploadInvestigateResponse,
    tags=["File Upload"],
    responses={400: {"model": ErrorResponse}},
)
async def upload_investigate(
    query: str = Form(..., description="Investigation query"),
    files: list[UploadFile] = File(..., description="Document files to analyze"),
    callback_url: Optional[str] = Form(None, description="Webhook URL for results"),
    keep_files: bool = Form(False, description="Keep files in S3 after processing"),
    background_tasks: BackgroundTasks = None,
):
    """Start investigation with uploaded files (async).

    Storage mode controlled by IRYS_STORAGE_MODE env var:
    - "local": Files stored on VM disk (for development)
    - "s3": Files streamed to S3 (for production, keeps VM light)

    Set keep_files=true to preserve files in S3 for later re-query (s3 mode only).
    """
    config = get_config()

    # Check concurrent job limit
    active_count = sum(
        1 for job in _jobs.values()
        if job.status in (JobStatus.PENDING, JobStatus.PROCESSING)
    )
    if active_count >= config.max_concurrent_jobs:
        raise HTTPException(
            status_code=429,
            detail=f"Too many concurrent jobs. Max: {config.max_concurrent_jobs}",
        )

    # Check file count
    if len(files) > config.max_documents_per_job:
        raise HTTPException(
            status_code=400,
            detail=f"Too many files ({len(files)}). Max: {config.max_documents_per_job}",
        )

    job_id = f"upload_{uuid.uuid4().hex[:12]}"

    try:
        # Read files into memory
        file_data = []
        for file in files:
            if not file.filename:
                continue
            content = await file.read()
            filename = Path(file.filename).name
            file_data.append((filename, content))

        if not file_data:
            raise HTTPException(status_code=400, detail="No valid files uploaded")

        # Branch based on storage mode
        if config.storage_mode == "local":
            # LOCAL MODE: Save to temp directory
            temp_dir = Path(config.temp_dir) / job_id
            temp_dir.mkdir(parents=True, exist_ok=True)
            for filename, content in file_data:
                (temp_dir / filename).write_bytes(content)
            s3_prefix = f"local:{job_id}"
        else:
            # S3 MODE: Upload to S3
            s3_repo = S3Repository(
                bucket=config.s3_bucket,
                prefix="",
                config=config,
            )
            s3_prefix = await s3_repo.upload_files(job_id, file_data)

        # Create job record
        job = JobResult(
            job_id=job_id,
            status=JobStatus.PENDING,
            query=query,
            s3_prefix=s3_prefix,
            created_at=datetime.now(),
        )
        _jobs[job_id] = job

        # Start background investigation
        background_tasks.add_task(
            _run_upload_investigation,
            job_id,
            query,
            s3_prefix,
            callback_url,
            keep_files,
            config,
        )

        return UploadInvestigateResponse(
            job_id=job_id,
            status=JobStatus.PENDING,
            message="Investigation started",
            files_received=len(file_data),
            estimated_seconds=60,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload investigation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def _run_upload_investigation(
    job_id: str,
    query: str,
    s3_prefix: str,
    callback_url: Optional[str],
    keep_files: bool,
    config: ServiceConfig,
):
    """Background task to run investigation on uploaded files."""
    job = _jobs[job_id]
    job.status = JobStatus.PROCESSING
    s3_repo = None
    temp_dir = None
    is_local = s3_prefix.startswith("local:")

    try:
        if is_local:
            # LOCAL MODE: Files already on disk
            temp_dir = Path(config.temp_dir) / job_id
        else:
            # S3 MODE: Download from S3
            s3_repo = S3Repository(
                bucket=config.s3_bucket,
                prefix=s3_prefix,
                config=config,
            )
            temp_dir = await s3_repo.download_to_temp(job_id)

        from irys import Irys
        irys = Irys(api_key=config.gemini_api_key)

        result = await irys.investigate(
            query=query,
            repository=str(temp_dir),
        )

        # Extract results
        job.analysis = result.output
        job.citations, job.entities = _serialize_result(result)
        job.documents_processed = result.state.documents_read
        job.status = JobStatus.COMPLETED
        job.completed_at = datetime.now()
        job.duration_seconds = (
            job.completed_at - job.created_at
        ).total_seconds()

        logger.info(f"Upload job {job_id} completed in {job.duration_seconds:.1f}s (mode={'local' if is_local else 's3'})")

        # Call webhook if provided
        if callback_url:
            await _send_callback(callback_url, job)

    except Exception as e:
        logger.error(f"Upload job {job_id} failed: {e}")
        job.status = JobStatus.FAILED
        job.error = str(e)
        job.completed_at = datetime.now()

    finally:
        if is_local:
            # LOCAL MODE: Delete temp directory
            import shutil
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir)
        else:
            # S3 MODE: Cleanup temp and optionally S3
            if s3_repo:
                await s3_repo.cleanup(job_id)
            if not keep_files:
                try:
                    cleanup_repo = S3Repository(
                        bucket=config.s3_bucket,
                        prefix="",
                        config=config,
                    )
                    await cleanup_repo.delete_prefix(s3_prefix)
                except Exception as e:
                    logger.warning(f"Failed to cleanup S3 prefix {s3_prefix}: {e}")


@app.post(
    "/upload/search",
    response_model=UploadSearchResponse,
    tags=["File Upload"],
)
async def upload_search(
    query: str = Form(..., description="Search query"),
    files: list[UploadFile] = File(..., description="Document files to search"),
    max_results: int = Form(20, ge=1, le=100, description="Maximum results"),
):
    """Quick search across uploaded files.

    Storage mode controlled by IRYS_STORAGE_MODE env var.
    Synchronous - returns results immediately.
    """
    config = get_config()
    job_id = f"uploadsearch_{uuid.uuid4().hex[:8]}"
    s3_prefix = None
    s3_repo = None
    temp_dir = None

    try:
        # Read files into memory
        file_data = []
        for file in files:
            if not file.filename:
                continue
            content = await file.read()
            filename = Path(file.filename).name
            file_data.append((filename, content))

        if not file_data:
            raise HTTPException(status_code=400, detail="No valid files uploaded")

        # Branch based on storage mode
        if config.storage_mode == "local":
            # LOCAL MODE: Save to temp directory
            temp_dir = Path(config.temp_dir) / job_id
            temp_dir.mkdir(parents=True, exist_ok=True)
            for filename, content in file_data:
                (temp_dir / filename).write_bytes(content)
        else:
            # S3 MODE: Upload to S3, then download to temp
            s3_repo = S3Repository(
                bucket=config.s3_bucket,
                prefix="",
                config=config,
            )
            s3_prefix = await s3_repo.upload_files(job_id, file_data)

            upload_repo = S3Repository(
                bucket=config.s3_bucket,
                prefix=s3_prefix,
                config=config,
            )
            temp_dir = await upload_repo.download_to_temp(job_id)

        # Run search
        from irys import quick_search as irys_search
        results = await irys_search(
            query=query,
            repository=str(temp_dir),
            api_key=config.gemini_api_key,
        )

        # Cleanup
        if config.storage_mode == "local":
            import shutil
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir)
        else:
            await s3_repo.cleanup(job_id)
            await s3_repo.delete_prefix(s3_prefix)

        return UploadSearchResponse(
            results=[SearchResult(**r) for r in results[:max_results]],
            total_matches=len(results),
            files_searched=len(file_data),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload search failed: {e}")
        # Cleanup on error
        if config.storage_mode == "local":
            import shutil
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir)
        elif s3_repo and s3_prefix:
            try:
                await s3_repo.delete_prefix(s3_prefix)
            except Exception:
                pass
        raise HTTPException(status_code=500, detail=str(e))


@app.post(
    "/upload/investigate/sync",
    response_model=SyncInvestigateResponse,
    tags=["File Upload"],
    responses={400: {"model": ErrorResponse}},
)
async def upload_investigate_sync(
    query: str = Form(..., description="Investigation query"),
    files: list[UploadFile] = File(..., description="Document files to analyze"),
    keep_files: bool = Form(False, description="Keep files in S3 after processing for re-query"),
):
    """Upload and investigate files synchronously.

    Storage mode controlled by IRYS_STORAGE_MODE env var:
    - "local": Files stored on VM disk (for development)
    - "s3": Files streamed to S3 (for production, keeps VM light)

    Set keep_files=true to preserve files in S3 for later re-query (only applies in s3 mode).

    Note: This endpoint blocks until investigation completes (may take 30-120 seconds).
    """
    config = get_config()
    job_id = f"sync_{uuid.uuid4().hex[:8]}"
    start_time = time.time()
    s3_prefix = None
    s3_repo = None
    temp_dir = None

    try:
        # Check file count
        if len(files) > config.max_documents_per_job:
            raise HTTPException(
                status_code=400,
                detail=f"Too many files ({len(files)}). Max: {config.max_documents_per_job}",
            )

        # Read files into memory
        file_data = []
        for file in files:
            if not file.filename:
                continue
            content = await file.read()
            filename = Path(file.filename).name
            file_data.append((filename, content))

        if not file_data:
            raise HTTPException(status_code=400, detail="No valid files uploaded")

        # Branch based on storage mode
        if config.storage_mode == "local":
            # LOCAL MODE: Save directly to temp directory
            temp_dir = Path(config.temp_dir) / job_id
            temp_dir.mkdir(parents=True, exist_ok=True)
            for filename, content in file_data:
                (temp_dir / filename).write_bytes(content)
        else:
            # S3 MODE: Upload to S3, then download to temp
            s3_repo = S3Repository(
                bucket=config.s3_bucket,
                prefix="",
                config=config,
            )
            s3_prefix = await s3_repo.upload_files(job_id, file_data)

            upload_repo = S3Repository(
                bucket=config.s3_bucket,
                prefix=s3_prefix,
                config=config,
            )
            temp_dir = await upload_repo.download_to_temp(job_id)

        # Run investigation
        from irys import Irys
        irys = Irys(api_key=config.gemini_api_key)

        result = await irys.investigate(
            query=query,
            repository=str(temp_dir),
        )

        # Cleanup temp files
        if config.storage_mode == "local":
            import shutil
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir)
        else:
            if s3_repo:
                await s3_repo.cleanup(job_id)
            # Cleanup S3 files (unless keep_files=True)
            if not keep_files and s3_repo and s3_prefix:
                await s3_repo.delete_prefix(s3_prefix)

        duration = time.time() - start_time
        logger.info(f"Sync investigation {job_id} completed in {duration:.1f}s (mode={config.storage_mode})")

        citations, entities = _serialize_result(result)
        response = SyncInvestigateResponse(
            query=query,
            analysis=result.output,
            citations=citations,
            entities=entities,
            documents_processed=result.state.documents_read,
            duration_seconds=round(duration, 2),
        )

        # Add S3 prefix to response if files kept (s3 mode only)
        if keep_files and s3_prefix:
            response.s3_prefix = s3_prefix

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Sync investigation failed: {e}")
        # Cleanup on error
        if config.storage_mode == "local":
            import shutil
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir)
        elif s3_repo and s3_prefix:
            try:
                await s3_repo.delete_prefix(s3_prefix)
            except Exception:
                pass
        raise HTTPException(status_code=500, detail=str(e))


# === S3 URL ENDPOINTS ===


@app.post(
    "/investigate/urls",
    response_model=InvestigateResponse,
    tags=["S3 URLs"],
    responses={400: {"model": ErrorResponse}},
)
async def investigate_urls(
    request: S3UrlsInvestigateRequest,
    background_tasks: BackgroundTasks,
):
    """Start investigation with specific S3 URLs.

    Provide a list of S3 URLs to download and analyze.
    Supports s3:// and https:// S3 URL formats.
    """
    config = get_config()

    # Check concurrent job limit
    active_count = sum(
        1 for job in _jobs.values()
        if job.status in (JobStatus.PENDING, JobStatus.PROCESSING)
    )
    if active_count >= config.max_concurrent_jobs:
        raise HTTPException(
            status_code=429,
            detail=f"Too many concurrent jobs. Max: {config.max_concurrent_jobs}",
        )

    # Generate job ID
    job_id = f"urls_{uuid.uuid4().hex[:12]}"

    # Create job record
    job = JobResult(
        job_id=job_id,
        status=JobStatus.PENDING,
        query=request.query,
        s3_prefix=f"urls:{len(request.s3_urls)} files",
        created_at=datetime.now(),
    )
    _jobs[job_id] = job

    # Start background investigation
    background_tasks.add_task(
        _run_urls_investigation,
        job_id,
        request,
        config,
    )

    return InvestigateResponse(
        job_id=job_id,
        status=JobStatus.PENDING,
        message=f"Investigation started for {len(request.s3_urls)} files",
        estimated_seconds=60,
    )


async def _run_urls_investigation(
    job_id: str,
    request: S3UrlsInvestigateRequest,
    config: ServiceConfig,
):
    """Background task to run investigation on S3 URLs."""
    job = _jobs[job_id]
    job.status = JobStatus.PROCESSING
    s3_repo = None
    temp_dir = None

    try:
        # Create S3 repository (bucket from first URL, or config default)
        s3_repo = S3Repository(
            bucket=config.s3_bucket or "placeholder",
            prefix="",
            config=config,
        )

        # Download documents from URLs
        temp_dir = await s3_repo.download_urls_to_temp(job_id, request.s3_urls)

        # Run investigation
        from irys import Irys
        irys = Irys(api_key=config.gemini_api_key)

        result = await irys.investigate(
            query=request.query,
            repository=str(temp_dir),
        )

        # Extract results
        job.analysis = result.output
        job.citations, job.entities = _serialize_result(result)
        job.documents_processed = result.state.documents_read
        job.status = JobStatus.COMPLETED
        job.completed_at = datetime.now()
        job.duration_seconds = (
            job.completed_at - job.created_at
        ).total_seconds()

        logger.info(f"URLs job {job_id} completed in {job.duration_seconds:.1f}s")

        # Call webhook if provided
        if request.callback_url:
            await _send_callback(request.callback_url, job)

    except Exception as e:
        logger.error(f"URLs job {job_id} failed: {e}")
        job.status = JobStatus.FAILED
        job.error = str(e)
        job.completed_at = datetime.now()

    finally:
        # Cleanup temp files
        if s3_repo and temp_dir:
            await s3_repo.cleanup(job_id)


@app.post(
    "/search/urls",
    response_model=SearchResponse,
    tags=["S3 URLs"],
)
async def search_urls(request: S3UrlsSearchRequest):
    """Quick keyword search across documents from S3 URLs.

    Provide a list of S3 URLs to download and search.
    Synchronous - returns results immediately.
    """
    config = get_config()
    job_id = f"searchurls_{uuid.uuid4().hex[:8]}"
    s3_repo = None

    try:
        # Create S3 repository
        s3_repo = S3Repository(
            bucket=config.s3_bucket or "placeholder",
            prefix="",
            config=config,
        )

        # Download documents from URLs
        temp_dir = await s3_repo.download_urls_to_temp(job_id, request.s3_urls)

        # Run search
        from irys import quick_search as irys_search
        results = await irys_search(
            query=request.query,
            repository=str(temp_dir),
            api_key=config.gemini_api_key,
        )

        # Cleanup
        await s3_repo.cleanup(job_id)

        return SearchResponse(
            results=[SearchResult(**r) for r in results[:request.max_results]],
            total_matches=len(results),
            documents_searched=len(request.s3_urls),
        )

    except Exception as e:
        logger.error(f"URL search failed: {e}")
        if s3_repo:
            await s3_repo.cleanup(job_id)
        raise HTTPException(status_code=500, detail=str(e))


@app.post(
    "/investigate/urls/sync",
    response_model=SyncInvestigateResponse,
    tags=["S3 URLs"],
    responses={400: {"model": ErrorResponse}},
)
async def investigate_urls_sync(request: S3UrlsInvestigateRequest):
    """Investigate documents from S3 URLs synchronously.

    Provide a list of S3 URLs to download and analyze.
    Returns complete results in a single request (blocks until done).
    Supports s3:// and https:// S3 URL formats.

    Note: This endpoint blocks until investigation completes (may take 30-120 seconds).
    """
    config = get_config()
    job_id = f"urlsync_{uuid.uuid4().hex[:8]}"
    start_time = time.time()
    s3_repo = None
    temp_dir = None

    try:
        # Check URL count
        if len(request.s3_urls) > config.max_documents_per_job:
            raise HTTPException(
                status_code=400,
                detail=f"Too many URLs ({len(request.s3_urls)}). Max: {config.max_documents_per_job}",
            )

        # Create S3 repository
        s3_repo = S3Repository(
            bucket=config.s3_bucket or "placeholder",
            prefix="",
            config=config,
        )

        # Download documents from URLs
        temp_dir = await s3_repo.download_urls_to_temp(job_id, request.s3_urls)

        # Run investigation
        from irys import Irys
        irys = Irys(api_key=config.gemini_api_key)

        result = await irys.investigate(
            query=request.query,
            repository=str(temp_dir),
        )

        # Cleanup
        await s3_repo.cleanup(job_id)

        duration = time.time() - start_time
        logger.info(f"URL sync investigation {job_id} completed in {duration:.1f}s")

        citations, entities = _serialize_result(result)
        return SyncInvestigateResponse(
            query=request.query,
            analysis=result.output,
            citations=citations,
            entities=entities,
            documents_processed=result.state.documents_read,
            duration_seconds=round(duration, 2),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"URL sync investigation failed: {e}")
        if s3_repo:
            await s3_repo.cleanup(job_id)
        raise HTTPException(status_code=500, detail=str(e))
